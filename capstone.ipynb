{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gb2imFdnAiZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Yuh2jlB-2HS",
        "outputId": "ddd86814-8a77-4519-d828-262ddb959ef4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0 symbol                       date    close     high     low  \\\n",
            "0           0   AAPL  2015-05-27 00:00:00+00:00  132.045  132.260  130.05   \n",
            "1           1   AAPL  2015-05-28 00:00:00+00:00  131.780  131.950  131.10   \n",
            "2           2   AAPL  2015-05-29 00:00:00+00:00  130.280  131.450  129.90   \n",
            "3           3   AAPL  2015-06-01 00:00:00+00:00  130.535  131.390  130.05   \n",
            "4           4   AAPL  2015-06-02 00:00:00+00:00  129.960  130.655  129.32   \n",
            "\n",
            "     open    volume    adjClose     adjHigh      adjLow     adjOpen  \\\n",
            "0  130.34  45833246  121.682558  121.880685  119.844118  120.111360   \n",
            "1  131.86  30733309  121.438354  121.595013  120.811718  121.512076   \n",
            "2  131.23  50884452  120.056069  121.134251  119.705890  120.931516   \n",
            "3  131.20  32112797  120.291057  121.078960  119.844118  120.903870   \n",
            "4  129.86  33667627  119.761181  120.401640  119.171406  119.669029   \n",
            "\n",
            "   adjVolume  divCash  splitFactor  Scaled Adj Close  \n",
            "0   45833246      0.0          1.0          0.152158  \n",
            "1   30733309      0.0          1.0          0.151146  \n",
            "2   50884452      0.0          1.0          0.145419  \n",
            "3   32112797      0.0          1.0          0.146393  \n",
            "4   33667627      0.0          1.0          0.144198  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the stock data into a Pandas DataFrame\n",
        "df = pd.read_csv('AAPL.csv')\n",
        "\n",
        "# Select the Adj Close column from the DataFrame\n",
        "adj_close = df['adjClose']\n",
        "\n",
        "# Calculate the minimum and maximum values of the Adj Close column\n",
        "min_value = adj_close.min()\n",
        "max_value = adj_close.max()\n",
        "\n",
        "# Perform min-max scaling by subtracting the minimum value and dividing by the range\n",
        "scaled_adj_close = (adj_close - min_value) / (max_value - min_value)\n",
        "\n",
        "# Add the scaled Adj Close column back to the DataFrame\n",
        "df['Scaled Adj Close'] = scaled_adj_close\n",
        "\n",
        "# Print the head of the DataFrame to verify the scaling\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "m-MJoaxsXGtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the mean of the Adj Close column\n",
        "mean_value = adj_close.mean()\n",
        "\n",
        "# Perform polarizing by subtracting the mean and dividing by the absolute value of the mean\n",
        "polarized_adj_close = (adj_close - mean_value) / abs(mean_value)\n",
        "\n",
        "# Add the polarized Adj Close column back to the DataFrame\n",
        "df['Polarized Adj Close'] = polarized_adj_close\n",
        "\n",
        "# Print the head of the DataFrame to verify the polarizing\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50qWgpTrAqcB",
        "outputId": "6c64464f-c03f-49b3-da66-d1c22128fbe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0 symbol                       date    close     high     low  \\\n",
            "0           0   AAPL  2015-05-27 00:00:00+00:00  132.045  132.260  130.05   \n",
            "1           1   AAPL  2015-05-28 00:00:00+00:00  131.780  131.950  131.10   \n",
            "2           2   AAPL  2015-05-29 00:00:00+00:00  130.280  131.450  129.90   \n",
            "3           3   AAPL  2015-06-01 00:00:00+00:00  130.535  131.390  130.05   \n",
            "4           4   AAPL  2015-06-02 00:00:00+00:00  129.960  130.655  129.32   \n",
            "\n",
            "     open    volume    adjClose     adjHigh      adjLow     adjOpen  \\\n",
            "0  130.34  45833246  121.682558  121.880685  119.844118  120.111360   \n",
            "1  131.86  30733309  121.438354  121.595013  120.811718  121.512076   \n",
            "2  131.23  50884452  120.056069  121.134251  119.705890  120.931516   \n",
            "3  131.20  32112797  120.291057  121.078960  119.844118  120.903870   \n",
            "4  129.86  33667627  119.761181  120.401640  119.171406  119.669029   \n",
            "\n",
            "   adjVolume  divCash  splitFactor  Scaled Adj Close  Polarized Adj Close  \n",
            "0   45833246      0.0          1.0          0.152158            -0.251952  \n",
            "1   30733309      0.0          1.0          0.151146            -0.253453  \n",
            "2   50884452      0.0          1.0          0.145419            -0.261951  \n",
            "3   32112797      0.0          1.0          0.146393            -0.260506  \n",
            "4   33667627      0.0          1.0          0.144198            -0.263763  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate the percentage change in the Adj Close values from one day to the next\n",
        "fluctuation_percentage = adj_close.pct_change() * 100\n",
        "\n",
        "# Add the fluctuation percentage column back to the DataFrame\n",
        "df['Fluctuation Percentage'] = fluctuation_percentage\n",
        "\n",
        "# Print the head of the DataFrame to verify the fluctuation percentage calculation\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SosHiqmJBKDp",
        "outputId": "75823632-346d-4a08-b04d-fd777907d850"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0 symbol                       date    close     high     low  \\\n",
            "0           0   AAPL  2015-05-27 00:00:00+00:00  132.045  132.260  130.05   \n",
            "1           1   AAPL  2015-05-28 00:00:00+00:00  131.780  131.950  131.10   \n",
            "2           2   AAPL  2015-05-29 00:00:00+00:00  130.280  131.450  129.90   \n",
            "3           3   AAPL  2015-06-01 00:00:00+00:00  130.535  131.390  130.05   \n",
            "4           4   AAPL  2015-06-02 00:00:00+00:00  129.960  130.655  129.32   \n",
            "\n",
            "     open    volume    adjClose     adjHigh      adjLow     adjOpen  \\\n",
            "0  130.34  45833246  121.682558  121.880685  119.844118  120.111360   \n",
            "1  131.86  30733309  121.438354  121.595013  120.811718  121.512076   \n",
            "2  131.23  50884452  120.056069  121.134251  119.705890  120.931516   \n",
            "3  131.20  32112797  120.291057  121.078960  119.844118  120.903870   \n",
            "4  129.86  33667627  119.761181  120.401640  119.171406  119.669029   \n",
            "\n",
            "   adjVolume  divCash  splitFactor  Scaled Adj Close  Polarized Adj Close  \\\n",
            "0   45833246      0.0          1.0          0.152158            -0.251952   \n",
            "1   30733309      0.0          1.0          0.151146            -0.253453   \n",
            "2   50884452      0.0          1.0          0.145419            -0.261951   \n",
            "3   32112797      0.0          1.0          0.146393            -0.260506   \n",
            "4   33667627      0.0          1.0          0.144198            -0.263763   \n",
            "\n",
            "   Fluctuation Percentage  \n",
            "0                     NaN  \n",
            "1               -0.200689  \n",
            "2               -1.138261  \n",
            "3                0.195732  \n",
            "4               -0.440495  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "metadata": {
        "id": "xNLy0LPcBg4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the scaled Adj Close column back to the DataFrame\n",
        "df['Scaled Adj Close'] = scaled_adj_close\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data, train_target, test_target = train_test_split(df[['Scaled Adj Close']], adj_close, test_size=0.2, random_state=0)\n",
        "\n",
        "# Train a linear regression model on the training data\n",
        "model = LinearRegression()\n",
        "model.fit(train_data, train_target)\n",
        "\n",
        "# Use the trained model to make predictions on the testing data\n",
        "predictions = model.predict(test_data)\n",
        "\n",
        "# Calculate the mean squared error between the actual and predicted values\n",
        "mse = mean_squared_error(test_target, predictions)\n",
        "\n",
        "# Print the mean squared error\n",
        "print('Mean Squared Error:', mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPJ3WkFuCn_o",
        "outputId": "346bcb3b-1ce5-45fe-b208-4748fd381205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 1.1571963399508735e-27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "K_7f8ki_FY8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the stock data into a Pandas DataFrame\n",
        "df = pd.read_csv('AAPL.csv')\n",
        "\n",
        "# Convert any columns that are of type 'object' or 'string' to a numerical data type\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == 'object' or df[col].dtype == 'string':\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# Perform min-max scaling on all columns in the DataFrame\n",
        "df_scaled = (df - df.min()) / (df.max() - df.min())\n",
        "\n",
        "# Print the head of the scaled DataFrame to verify the scaling\n",
        "print(df_scaled.head())\n"
      ],
      "metadata": {
        "id": "855CuVdbHjIv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e64bc9c-f346-4eaa-c1ba-38422c3641cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0  symbol  date     close      high       low      open    volume  \\\n",
            "0    0.000000     NaN   NaN  0.176074  0.171860  0.173508  0.171857  0.228522   \n",
            "1    0.000796     NaN   NaN  0.174956  0.170548  0.177997  0.178333  0.128419   \n",
            "2    0.001591     NaN   NaN  0.168623  0.168431  0.172866  0.175649  0.262008   \n",
            "3    0.002387     NaN   NaN  0.169699  0.168177  0.173508  0.175521  0.137564   \n",
            "4    0.003182     NaN   NaN  0.167272  0.165065  0.170387  0.169812  0.147872   \n",
            "\n",
            "   adjClose   adjHigh    adjLow   adjOpen  adjVolume  divCash  splitFactor  \n",
            "0  0.152158  0.148554  0.149806  0.148290   0.228522      0.0          NaN  \n",
            "1  0.151146  0.147365  0.153865  0.154145   0.128419      0.0          NaN  \n",
            "2  0.145419  0.145446  0.149226  0.151718   0.262008      0.0          NaN  \n",
            "3  0.146393  0.145216  0.149806  0.151603   0.137564      0.0          NaN  \n",
            "4  0.144198  0.142396  0.146984  0.146441   0.147872      0.0          NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "0N1DOoV_QDSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize PCA with 1 component\n",
        "pca = PCA(n_components=1)\n",
        "\n",
        "# Fit and transform the scaled Adj Close data\n",
        "pca_adj_close = pca.fit_transform(scaled_adj_close.values.reshape(-1, 1))\n",
        "\n",
        "# Add the PCA Adj Close column back to the DataFrame\n",
        "df['PCA Adj Close'] = pca_adj_close\n",
        "\n",
        "# Print the head of the DataFrame to verify the PCA transformation\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dd8-xVubMZEn",
        "outputId": "545aac41-7ead-410e-dedf-982f918017da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0 symbol                       date    close     high     low  \\\n",
            "0           0   AAPL  2015-05-27 00:00:00+00:00  132.045  132.260  130.05   \n",
            "1           1   AAPL  2015-05-28 00:00:00+00:00  131.780  131.950  131.10   \n",
            "2           2   AAPL  2015-05-29 00:00:00+00:00  130.280  131.450  129.90   \n",
            "3           3   AAPL  2015-06-01 00:00:00+00:00  130.535  131.390  130.05   \n",
            "4           4   AAPL  2015-06-02 00:00:00+00:00  129.960  130.655  129.32   \n",
            "\n",
            "     open    volume    adjClose     adjHigh      adjLow     adjOpen  \\\n",
            "0  130.34  45833246  121.682558  121.880685  119.844118  120.111360   \n",
            "1  131.86  30733309  121.438354  121.595013  120.811718  121.512076   \n",
            "2  131.23  50884452  120.056069  121.134251  119.705890  120.931516   \n",
            "3  131.20  32112797  120.291057  121.078960  119.844118  120.903870   \n",
            "4  129.86  33667627  119.761181  120.401640  119.171406  119.669029   \n",
            "\n",
            "   adjVolume  divCash  splitFactor  PCA Adj Close  \n",
            "0   45833246      0.0          1.0      -0.169789  \n",
            "1   30733309      0.0          1.0      -0.170801  \n",
            "2   50884452      0.0          1.0      -0.176527  \n",
            "3   32112797      0.0          1.0      -0.175554  \n",
            "4   33667627      0.0          1.0      -0.177749  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "\n",
        "\n",
        "\n",
        "# Convert the data into a format suitable for LSTM\n",
        "timesteps = 90 # Number of previous days used to predict the next day's price\n",
        "X = []\n",
        "y = []\n",
        "for i in range(timesteps, len(adj_close_scaled)):\n",
        "    X.append(adj_close_scaled[i-timesteps:i, 0])\n",
        "    y.append(adj_close_scaled[i, 0])\n",
        "X, y = np.array(X), np.array(y)\n",
        "\n",
        "# Reshape the data for LSTM input\n",
        "X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, return_sequences=True, input_shape=(X.shape[1], 1)))\n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=100, batch_size=32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ey5ZQG9fQIb8",
        "outputId": "f4241c97-82a0-4778-8150-219e65935e08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "37/37 [==============================] - 10s 111ms/step - loss: 0.0273\n",
            "Epoch 2/100\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.0017\n",
            "Epoch 3/100\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.0012\n",
            "Epoch 4/100\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.0011\n",
            "Epoch 5/100\n",
            "37/37 [==============================] - 4s 119ms/step - loss: 0.0011\n",
            "Epoch 6/100\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 9.3322e-04\n",
            "Epoch 7/100\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 9.1365e-04\n",
            "Epoch 8/100\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 7.8014e-04\n",
            "Epoch 9/100\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 8.2402e-04\n",
            "Epoch 10/100\n",
            "37/37 [==============================] - 3s 86ms/step - loss: 7.0653e-04\n",
            "Epoch 11/100\n",
            "37/37 [==============================] - 3s 92ms/step - loss: 9.1430e-04\n",
            "Epoch 12/100\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 6.5848e-04\n",
            "Epoch 13/100\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 6.4685e-04\n",
            "Epoch 14/100\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 6.7616e-04\n",
            "Epoch 15/100\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 8.0588e-04\n",
            "Epoch 16/100\n",
            "37/37 [==============================] - 3s 87ms/step - loss: 7.9963e-04\n",
            "Epoch 17/100\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 6.2199e-04\n",
            "Epoch 18/100\n",
            "37/37 [==============================] - 4s 121ms/step - loss: 6.4489e-04\n",
            "Epoch 19/100\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 5.8758e-04\n",
            "Epoch 20/100\n",
            "37/37 [==============================] - 3s 85ms/step - loss: 6.1966e-04\n",
            "Epoch 21/100\n",
            "37/37 [==============================] - 3s 88ms/step - loss: 6.0189e-04\n",
            "Epoch 22/100\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 5.6979e-04\n",
            "Epoch 23/100\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 5.9644e-04\n",
            "Epoch 24/100\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 6.3738e-04\n",
            "Epoch 25/100\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 6.1298e-04\n",
            "Epoch 26/100\n",
            "37/37 [==============================] - 3s 84ms/step - loss: 5.4613e-04\n",
            "Epoch 27/100\n",
            "37/37 [==============================] - 4s 112ms/step - loss: 6.5199e-04\n",
            "Epoch 28/100\n",
            "37/37 [==============================] - 3s 86ms/step - loss: 6.0166e-04\n",
            "Epoch 29/100\n",
            "37/37 [==============================] - 3s 92ms/step - loss: 5.5839e-04\n",
            "Epoch 30/100\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 5.2398e-04\n",
            "Epoch 31/100\n",
            "37/37 [==============================] - 4s 117ms/step - loss: 7.3404e-04\n",
            "Epoch 32/100\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 5.2930e-04\n",
            "Epoch 33/100\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 5.4441e-04\n",
            "Epoch 34/100\n",
            "37/37 [==============================] - 7s 203ms/step - loss: 4.7623e-04\n",
            "Epoch 35/100\n",
            "37/37 [==============================] - 6s 165ms/step - loss: 4.9651e-04\n",
            "Epoch 36/100\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 6.7368e-04\n",
            "Epoch 37/100\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 4.8714e-04\n",
            "Epoch 38/100\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 4.6370e-04\n",
            "Epoch 39/100\n",
            "37/37 [==============================] - 3s 84ms/step - loss: 4.9146e-04\n",
            "Epoch 40/100\n",
            "37/37 [==============================] - 5s 122ms/step - loss: 4.8914e-04\n",
            "Epoch 41/100\n",
            "37/37 [==============================] - 3s 87ms/step - loss: 4.6406e-04\n",
            "Epoch 42/100\n",
            "37/37 [==============================] - 3s 90ms/step - loss: 4.5376e-04\n",
            "Epoch 43/100\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 4.8784e-04\n",
            "Epoch 44/100\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 4.4780e-04\n",
            "Epoch 45/100\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 4.3581e-04\n",
            "Epoch 46/100\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 4.4052e-04\n",
            "Epoch 47/100\n",
            "37/37 [==============================] - 4s 115ms/step - loss: 5.3847e-04\n",
            "Epoch 48/100\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 4.5768e-04\n",
            "Epoch 49/100\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 4.5872e-04\n",
            "Epoch 50/100\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 4.2896e-04\n",
            "Epoch 51/100\n",
            "37/37 [==============================] - 4s 121ms/step - loss: 4.5033e-04\n",
            "Epoch 52/100\n",
            "37/37 [==============================] - 3s 91ms/step - loss: 4.2805e-04\n",
            "Epoch 53/100\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 3.9710e-04\n",
            "Epoch 54/100\n",
            "37/37 [==============================] - 4s 113ms/step - loss: 4.3705e-04\n",
            "Epoch 55/100\n",
            "37/37 [==============================] - 3s 92ms/step - loss: 3.8791e-04\n",
            "Epoch 56/100\n",
            "37/37 [==============================] - 3s 93ms/step - loss: 3.8648e-04\n",
            "Epoch 57/100\n",
            "37/37 [==============================] - 3s 84ms/step - loss: 3.6148e-04\n",
            "Epoch 58/100\n",
            "37/37 [==============================] - 4s 114ms/step - loss: 5.1290e-04\n",
            "Epoch 59/100\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 3.5046e-04\n",
            "Epoch 60/100\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 3.3523e-04\n",
            "Epoch 61/100\n",
            "37/37 [==============================] - 4s 117ms/step - loss: 4.3314e-04\n",
            "Epoch 62/100\n",
            "37/37 [==============================] - 3s 89ms/step - loss: 3.5438e-04\n",
            "Epoch 63/100\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 3.3802e-04\n",
            "Epoch 64/100\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 3.3318e-04\n",
            "Epoch 65/100\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 3.1431e-04\n",
            "Epoch 66/100\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 4.0131e-04\n",
            "Epoch 67/100\n",
            "37/37 [==============================] - 3s 88ms/step - loss: 2.9352e-04\n",
            "Epoch 68/100\n",
            "37/37 [==============================] - 3s 88ms/step - loss: 3.0683e-04\n",
            "Epoch 69/100\n",
            "37/37 [==============================] - 4s 110ms/step - loss: 2.9693e-04\n",
            "Epoch 70/100\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 3.8630e-04\n",
            "Epoch 71/100\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 3.1762e-04\n",
            "Epoch 72/100\n",
            "37/37 [==============================] - 3s 93ms/step - loss: 2.9586e-04\n",
            "Epoch 73/100\n",
            "37/37 [==============================] - 4s 110ms/step - loss: 2.8803e-04\n",
            "Epoch 74/100\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 3.1338e-04\n",
            "Epoch 75/100\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 3.3326e-04\n",
            "Epoch 76/100\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 3.4661e-04\n",
            "Epoch 77/100\n",
            "37/37 [==============================] - 4s 109ms/step - loss: 2.8347e-04\n",
            "Epoch 78/100\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 2.8617e-04\n",
            "Epoch 79/100\n",
            "37/37 [==============================] - 3s 86ms/step - loss: 2.6675e-04\n",
            "Epoch 80/100\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 2.9375e-04\n",
            "Epoch 81/100\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 3.4705e-04\n",
            "Epoch 82/100\n",
            "37/37 [==============================] - 3s 87ms/step - loss: 3.1351e-04\n",
            "Epoch 83/100\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 3.1687e-04\n",
            "Epoch 84/100\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 2.9353e-04\n",
            "Epoch 85/100\n",
            "37/37 [==============================] - 3s 90ms/step - loss: 2.8284e-04\n",
            "Epoch 86/100\n",
            "37/37 [==============================] - 3s 85ms/step - loss: 2.6110e-04\n",
            "Epoch 87/100\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 2.8637e-04\n",
            "Epoch 88/100\n",
            "37/37 [==============================] - 4s 113ms/step - loss: 3.3438e-04\n",
            "Epoch 89/100\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 2.6222e-04\n",
            "Epoch 90/100\n",
            "37/37 [==============================] - 3s 89ms/step - loss: 2.9323e-04\n",
            "Epoch 91/100\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 2.7587e-04\n",
            "Epoch 92/100\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 2.6724e-04\n",
            "Epoch 93/100\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 3.0119e-04\n",
            "Epoch 94/100\n",
            "37/37 [==============================] - 3s 88ms/step - loss: 2.7703e-04\n",
            "Epoch 95/100\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 2.8354e-04\n",
            "Epoch 96/100\n",
            "37/37 [==============================] - 4s 118ms/step - loss: 2.6572e-04\n",
            "Epoch 97/100\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 2.8656e-04\n",
            "Epoch 98/100\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 2.7562e-04\n",
            "Epoch 99/100\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 2.6984e-04\n",
            "Epoch 100/100\n",
            "37/37 [==============================] - 3s 91ms/step - loss: 2.6850e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7effb4596460>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the trained model to make predictions\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# Print the predictions\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObUbXZAwSWVV",
        "outputId": "76cc6d1b-239c-4352-a0f9-5866da5442a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37/37 [==============================] - 3s 20ms/step\n",
            "[[0.06754002]\n",
            " [0.0668511 ]\n",
            " [0.06770711]\n",
            " ...\n",
            " [0.95402664]\n",
            " [0.9709049 ]\n",
            " [0.9669602 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error"
      ],
      "metadata": {
        "id": "aWqUYdbDgVkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(\"MSE: \", mse)\n",
        "print(\"MAE: \", mae)\n",
        "print(\"RMSE: \", rmse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvfdnw8Vd7Br",
        "outputId": "500027ef-4f2a-44ef-af0c-e9136eb36042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "30/30 [==============================] - 3s 91ms/step - loss: 2.6589e-04 - val_loss: 2.4027e-04\n",
            "Epoch 2/100\n",
            "30/30 [==============================] - 4s 117ms/step - loss: 2.5631e-04 - val_loss: 2.5689e-04\n",
            "Epoch 3/100\n",
            "30/30 [==============================] - 2s 83ms/step - loss: 2.8339e-04 - val_loss: 3.7502e-04\n",
            "Epoch 4/100\n",
            "30/30 [==============================] - 3s 84ms/step - loss: 2.6152e-04 - val_loss: 2.1210e-04\n",
            "Epoch 5/100\n",
            "30/30 [==============================] - 2s 82ms/step - loss: 2.7452e-04 - val_loss: 2.2130e-04\n",
            "Epoch 6/100\n",
            "30/30 [==============================] - 3s 110ms/step - loss: 2.9236e-04 - val_loss: 2.4266e-04\n",
            "Epoch 7/100\n",
            "30/30 [==============================] - 3s 92ms/step - loss: 2.5290e-04 - val_loss: 2.2844e-04\n",
            "Epoch 8/100\n",
            "30/30 [==============================] - 2s 83ms/step - loss: 2.4902e-04 - val_loss: 2.1555e-04\n",
            "Epoch 9/100\n",
            "30/30 [==============================] - 3s 83ms/step - loss: 2.5713e-04 - val_loss: 4.8736e-04\n",
            "Epoch 10/100\n",
            "30/30 [==============================] - 3s 91ms/step - loss: 3.6468e-04 - val_loss: 2.6077e-04\n",
            "Epoch 11/100\n",
            "30/30 [==============================] - 5s 151ms/step - loss: 2.5119e-04 - val_loss: 2.1898e-04\n",
            "Epoch 12/100\n",
            "30/30 [==============================] - 3s 93ms/step - loss: 2.4049e-04 - val_loss: 2.2268e-04\n",
            "Epoch 13/100\n",
            "30/30 [==============================] - 3s 90ms/step - loss: 2.5594e-04 - val_loss: 2.4657e-04\n",
            "Epoch 14/100\n",
            "30/30 [==============================] - 2s 83ms/step - loss: 2.6447e-04 - val_loss: 2.1195e-04\n",
            "Epoch 15/100\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 2.3992e-04 - val_loss: 2.7550e-04\n",
            "Epoch 16/100\n",
            "30/30 [==============================] - 3s 89ms/step - loss: 2.5507e-04 - val_loss: 2.0192e-04\n",
            "Epoch 17/100\n",
            "30/30 [==============================] - 2s 82ms/step - loss: 3.0349e-04 - val_loss: 2.8287e-04\n",
            "Epoch 18/100\n",
            "30/30 [==============================] - 3s 84ms/step - loss: 2.4859e-04 - val_loss: 2.0863e-04\n",
            "Epoch 19/100\n",
            "30/30 [==============================] - 3s 100ms/step - loss: 2.6107e-04 - val_loss: 2.1458e-04\n",
            "Epoch 20/100\n",
            "30/30 [==============================] - 3s 101ms/step - loss: 2.5525e-04 - val_loss: 2.7081e-04\n",
            "Epoch 21/100\n",
            "30/30 [==============================] - 3s 88ms/step - loss: 2.4277e-04 - val_loss: 2.3038e-04\n",
            "Epoch 22/100\n",
            "30/30 [==============================] - 2s 82ms/step - loss: 3.3817e-04 - val_loss: 2.7944e-04\n",
            "Epoch 23/100\n",
            "30/30 [==============================] - 2s 81ms/step - loss: 2.7714e-04 - val_loss: 2.0700e-04\n",
            "Epoch 24/100\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 2.5316e-04 - val_loss: 2.5607e-04\n",
            "Epoch 25/100\n",
            "30/30 [==============================] - 3s 85ms/step - loss: 2.5714e-04 - val_loss: 2.1745e-04\n",
            "Epoch 26/100\n",
            "30/30 [==============================] - 3s 86ms/step - loss: 2.5129e-04 - val_loss: 2.7902e-04\n",
            "Epoch 27/100\n",
            "30/30 [==============================] - 2s 81ms/step - loss: 2.6621e-04 - val_loss: 2.1055e-04\n",
            "Epoch 28/100\n",
            "30/30 [==============================] - 3s 88ms/step - loss: 2.6103e-04 - val_loss: 2.1986e-04\n",
            "Epoch 29/100\n",
            "30/30 [==============================] - 3s 109ms/step - loss: 2.7778e-04 - val_loss: 2.9967e-04\n",
            "Epoch 30/100\n",
            "30/30 [==============================] - 2s 81ms/step - loss: 2.6847e-04 - val_loss: 2.6353e-04\n",
            "Epoch 31/100\n",
            "30/30 [==============================] - 2s 82ms/step - loss: 2.4741e-04 - val_loss: 2.9777e-04\n",
            "Epoch 32/100\n",
            "30/30 [==============================] - 2s 81ms/step - loss: 2.8590e-04 - val_loss: 2.3301e-04\n",
            "Epoch 33/100\n",
            "30/30 [==============================] - 3s 102ms/step - loss: 2.8001e-04 - val_loss: 2.1180e-04\n",
            "Epoch 34/100\n",
            "30/30 [==============================] - 3s 95ms/step - loss: 2.4369e-04 - val_loss: 2.6488e-04\n",
            "Epoch 35/100\n",
            "30/30 [==============================] - 2s 81ms/step - loss: 2.4324e-04 - val_loss: 2.1977e-04\n",
            "Epoch 36/100\n",
            "30/30 [==============================] - 2s 82ms/step - loss: 3.7503e-04 - val_loss: 2.4121e-04\n",
            "Epoch 37/100\n",
            "30/30 [==============================] - 3s 97ms/step - loss: 2.5817e-04 - val_loss: 2.3197e-04\n",
            "Epoch 38/100\n",
            "30/30 [==============================] - 4s 131ms/step - loss: 2.4745e-04 - val_loss: 2.3327e-04\n",
            "Epoch 39/100\n",
            "30/30 [==============================] - 2s 80ms/step - loss: 2.6615e-04 - val_loss: 2.0981e-04\n",
            "Epoch 40/100\n",
            "30/30 [==============================] - 2s 83ms/step - loss: 2.5580e-04 - val_loss: 2.9418e-04\n",
            "Epoch 41/100\n",
            "30/30 [==============================] - 2s 82ms/step - loss: 2.8160e-04 - val_loss: 2.0977e-04\n",
            "Epoch 42/100\n",
            "30/30 [==============================] - 3s 92ms/step - loss: 2.9112e-04 - val_loss: 2.7930e-04\n",
            "Epoch 43/100\n",
            "30/30 [==============================] - 3s 107ms/step - loss: 2.7018e-04 - val_loss: 2.0840e-04\n",
            "Epoch 44/100\n",
            "30/30 [==============================] - 2s 83ms/step - loss: 2.3792e-04 - val_loss: 2.0997e-04\n",
            "Epoch 45/100\n",
            "30/30 [==============================] - 2s 83ms/step - loss: 2.4009e-04 - val_loss: 2.4470e-04\n",
            "Epoch 46/100\n",
            "30/30 [==============================] - 3s 89ms/step - loss: 2.6113e-04 - val_loss: 2.2184e-04\n",
            "Epoch 47/100\n",
            "30/30 [==============================] - 3s 115ms/step - loss: 2.4766e-04 - val_loss: 2.2060e-04\n",
            "Epoch 48/100\n",
            "30/30 [==============================] - 3s 84ms/step - loss: 2.7407e-04 - val_loss: 2.3245e-04\n",
            "Epoch 49/100\n",
            "30/30 [==============================] - 3s 83ms/step - loss: 2.7459e-04 - val_loss: 4.3103e-04\n",
            "Epoch 50/100\n",
            "30/30 [==============================] - 2s 81ms/step - loss: 2.8360e-04 - val_loss: 2.1362e-04\n",
            "Epoch 51/100\n",
            "30/30 [==============================] - 2s 81ms/step - loss: 2.8696e-04 - val_loss: 3.2383e-04\n",
            "Epoch 52/100\n",
            "30/30 [==============================] - 4s 119ms/step - loss: 2.7071e-04 - val_loss: 2.0927e-04\n",
            "Epoch 53/100\n",
            "30/30 [==============================] - 2s 81ms/step - loss: 2.4726e-04 - val_loss: 2.0489e-04\n",
            "Epoch 54/100\n",
            "30/30 [==============================] - 2s 82ms/step - loss: 2.4813e-04 - val_loss: 3.1186e-04\n",
            "Epoch 55/100\n",
            "30/30 [==============================] - 3s 117ms/step - loss: 3.2035e-04 - val_loss: 2.2460e-04\n",
            "Epoch 56/100\n",
            "30/30 [==============================] - 3s 118ms/step - loss: 2.4853e-04 - val_loss: 2.0496e-04\n",
            "Epoch 57/100\n",
            "30/30 [==============================] - 3s 83ms/step - loss: 2.5586e-04 - val_loss: 2.1337e-04\n",
            "Epoch 58/100\n",
            "30/30 [==============================] - 3s 83ms/step - loss: 2.5313e-04 - val_loss: 2.0694e-04\n",
            "Epoch 59/100\n",
            "30/30 [==============================] - 2s 83ms/step - loss: 2.9644e-04 - val_loss: 2.1157e-04\n",
            "Epoch 60/100\n",
            "30/30 [==============================] - 2s 83ms/step - loss: 2.4073e-04 - val_loss: 2.5361e-04\n",
            "Epoch 61/100\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 2.8055e-04 - val_loss: 2.2206e-04\n",
            "Epoch 62/100\n",
            "30/30 [==============================] - 3s 85ms/step - loss: 2.4518e-04 - val_loss: 2.1516e-04\n",
            "Epoch 63/100\n",
            "30/30 [==============================] - 3s 83ms/step - loss: 3.0225e-04 - val_loss: 2.9731e-04\n",
            "Epoch 64/100\n",
            "30/30 [==============================] - 3s 91ms/step - loss: 3.0629e-04 - val_loss: 2.4771e-04\n",
            "Epoch 65/100\n",
            "30/30 [==============================] - 3s 113ms/step - loss: 2.7408e-04 - val_loss: 2.6908e-04\n",
            "Epoch 66/100\n",
            "30/30 [==============================] - 3s 91ms/step - loss: 3.2354e-04 - val_loss: 2.3043e-04\n",
            "Epoch 67/100\n",
            "30/30 [==============================] - 3s 84ms/step - loss: 2.6205e-04 - val_loss: 2.1413e-04\n",
            "Epoch 68/100\n",
            "30/30 [==============================] - 3s 90ms/step - loss: 2.5218e-04 - val_loss: 2.1058e-04\n",
            "Epoch 69/100\n",
            "30/30 [==============================] - 3s 85ms/step - loss: 2.6720e-04 - val_loss: 2.0390e-04\n",
            "Epoch 70/100\n",
            "30/30 [==============================] - 4s 125ms/step - loss: 2.7919e-04 - val_loss: 2.9836e-04\n",
            "Epoch 71/100\n",
            "30/30 [==============================] - 3s 92ms/step - loss: 2.5806e-04 - val_loss: 2.3568e-04\n",
            "Epoch 72/100\n",
            "30/30 [==============================] - 3s 92ms/step - loss: 2.3893e-04 - val_loss: 2.0640e-04\n",
            "Epoch 73/100\n",
            "30/30 [==============================] - 3s 84ms/step - loss: 2.4823e-04 - val_loss: 2.4214e-04\n",
            "Epoch 74/100\n",
            "30/30 [==============================] - 3s 117ms/step - loss: 2.3413e-04 - val_loss: 2.2086e-04\n",
            "Epoch 75/100\n",
            "30/30 [==============================] - 3s 84ms/step - loss: 2.3553e-04 - val_loss: 2.1139e-04\n",
            "Epoch 76/100\n",
            "30/30 [==============================] - 3s 91ms/step - loss: 2.3752e-04 - val_loss: 2.2002e-04\n",
            "Epoch 77/100\n",
            "30/30 [==============================] - 3s 91ms/step - loss: 2.4270e-04 - val_loss: 2.1246e-04\n",
            "Epoch 78/100\n",
            "30/30 [==============================] - 3s 91ms/step - loss: 2.3951e-04 - val_loss: 2.1886e-04\n",
            "Epoch 79/100\n",
            "30/30 [==============================] - 4s 117ms/step - loss: 2.3469e-04 - val_loss: 2.8031e-04\n",
            "Epoch 80/100\n",
            "30/30 [==============================] - 3s 92ms/step - loss: 2.4369e-04 - val_loss: 2.1584e-04\n",
            "Epoch 81/100\n",
            "30/30 [==============================] - 3s 100ms/step - loss: 2.5554e-04 - val_loss: 2.7767e-04\n",
            "Epoch 82/100\n",
            "30/30 [==============================] - 3s 85ms/step - loss: 2.4310e-04 - val_loss: 2.9103e-04\n",
            "Epoch 83/100\n",
            "30/30 [==============================] - 5s 172ms/step - loss: 2.3742e-04 - val_loss: 2.7456e-04\n",
            "Epoch 84/100\n",
            "30/30 [==============================] - 3s 84ms/step - loss: 2.4025e-04 - val_loss: 2.1179e-04\n",
            "Epoch 85/100\n",
            "30/30 [==============================] - 3s 87ms/step - loss: 2.4055e-04 - val_loss: 2.0304e-04\n",
            "Epoch 86/100\n",
            "30/30 [==============================] - 3s 87ms/step - loss: 2.3097e-04 - val_loss: 2.2216e-04\n",
            "Epoch 87/100\n",
            "30/30 [==============================] - 4s 134ms/step - loss: 2.4718e-04 - val_loss: 2.0835e-04\n",
            "Epoch 88/100\n",
            "30/30 [==============================] - 3s 99ms/step - loss: 2.3414e-04 - val_loss: 2.0742e-04\n",
            "Epoch 89/100\n",
            "30/30 [==============================] - 3s 88ms/step - loss: 2.4153e-04 - val_loss: 2.3167e-04\n",
            "Epoch 90/100\n",
            "30/30 [==============================] - 3s 92ms/step - loss: 2.5400e-04 - val_loss: 2.1509e-04\n",
            "Epoch 91/100\n",
            "30/30 [==============================] - 4s 126ms/step - loss: 2.7488e-04 - val_loss: 2.0674e-04\n",
            "Epoch 92/100\n",
            "30/30 [==============================] - 3s 84ms/step - loss: 2.4423e-04 - val_loss: 2.3399e-04\n",
            "Epoch 93/100\n",
            "30/30 [==============================] - 3s 90ms/step - loss: 2.4480e-04 - val_loss: 3.7825e-04\n",
            "Epoch 94/100\n",
            "30/30 [==============================] - 2s 83ms/step - loss: 2.8447e-04 - val_loss: 2.0229e-04\n",
            "Epoch 95/100\n",
            "30/30 [==============================] - 3s 91ms/step - loss: 2.7406e-04 - val_loss: 2.8238e-04\n",
            "Epoch 96/100\n",
            "30/30 [==============================] - 3s 113ms/step - loss: 2.3116e-04 - val_loss: 2.0089e-04\n",
            "Epoch 97/100\n",
            "30/30 [==============================] - 2s 82ms/step - loss: 2.2718e-04 - val_loss: 2.6265e-04\n",
            "Epoch 98/100\n",
            "30/30 [==============================] - 2s 82ms/step - loss: 2.5405e-04 - val_loss: 2.0621e-04\n",
            "Epoch 99/100\n",
            "30/30 [==============================] - 2s 82ms/step - loss: 2.4858e-04 - val_loss: 2.1788e-04\n",
            "Epoch 100/100\n",
            "30/30 [==============================] - 3s 112ms/step - loss: 2.3886e-04 - val_loss: 3.1556e-04\n",
            "8/8 [==============================] - 0s 28ms/step\n",
            "MSE:  0.0003155642467394468\n",
            "MAE:  0.010997769233692547\n",
            "RMSE:  0.01776412808835398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = adj_close_scaled[timesteps:]\n",
        "X_test, y_test = X[-len(test_set):], y[-len(test_set):]\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "mse = model.evaluate(X_test, y_test, verbose=0)\n",
        "accuracy = 100 - (mse * 100)\n",
        "print('Accuracy: %.2f%%' % accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1bwH8ubgUf6",
        "outputId": "29fc5212-905e-48e7-b1ad-3a039eae48a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 99.97%\n"
          ]
        }
      ]
    }
  ]
}